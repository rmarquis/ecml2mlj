\label{sec:inference}

We infer skill distributions in all proposed models via
online Bayesian updating.  While exact inference in the purely Gaussian
models can be achieved by solving linear systems, Bayesian
updating provides an efficient (also exact) incremental learning alternative.
Equations for Bayesian updates and the probability of three possible match outcomes (e.g., winning, losing and drawing) are
model-dependent and presented below.

\subsection{Inference in TrueSkill}

\subsubsection{Bayesian update}
The Bayesian update equations in the TrueSkill model
(Figure~\ref{fig:trueskill}) are presented
in~\cite{herbrich06569}, and we omit the details in the present paper and refer the interesting readers to Table 1 in~\cite{herbrich06569}. 

\subsubsection{Win/Lose/Draw Probability} 
\label{sec:winProbabilityTrueSkill}
Given skill levels of team $i$ and $j$, $l_i\sim\mathcal{N}(l_i;\mu_i,\sigma_i^2)$ and
$l_j\sim\mathcal{N}(l_j;\mu_j,\sigma_j^2)$, we first compute the
distribution over performance difference variable $d$, and get
$d\sim\mathcal{N}(d;\mu_d,\sigma_d^2)$ with $\mu_d = \mu_i - \mu_j$
and $\sigma_d^2 = \sigma_i^2 + \sigma_j^2+2\beta^2$. The winning
probability of team $i$ is given by the probability $p(d>0)$ defined as
\begin{align}
  p(d>0) = 1 - \Phi\left(\frac{-\mu_d}{\sigma_d}\right),
\end{align}
where $\Phi(\cdot)$ is the normal CDF. Likewise, we define the draw probability as the probability distribution funciton of the variable $d$, evaluated at 0, and the lose probability of team $i$ as $\Phi\left(\frac{-\mu_d}{\sigma_d}\right)$.

\subsection{Inference in Poisson-OD Model}
\label{sec:PoissonInference}

\subsubsection{Bayes Update}
Some of the update equations in the Poisson-OD model
(Figure~\ref{fig:trueskill_variant}) have been presented in
\cite{herbrich06569}, with the exception of the marginal distribution
over $x$ and the message passing from the Poisson factor to $x$. Given
a prior Gaussian distribution over $x$, $\mathcal{N}(x;\mu,
\sigma^2)$, we next
demonstrate how to update the belief on $x$ when observing
team $i$'s score $s_i$.

By the sum-product algorithm \cite{kschischang01498}, the
marginal distribution of $x$ is given by a product of messages
\begin{align}\label{eq:marginal}
    p(x|s_i) = m_{\delta \rightarrow x}(x) m_{s_i \rightarrow x}(x).
\end{align}
\unindent To avoid cluttered notation, let us use $m_1(x)$ to represent
$m_{\delta \rightarrow x}(x) = \mathcal{N}(x;\mu,\sigma^2)$, i.e., the message
passing from the factor $\delta(\cdot)$ to $x$, and $m_2(x)$ for
$m_{s_i\rightarrow x}(x) = Poisson(s_i;\exp(x))$, i.e.,
the message passing from the Poisson
factor to $x$ (c.f., messages labeled 1 and 2 in
Figure~\ref{fig:trueskill_variant}). Due to the multiplication of
$m_{1}(x)$ and $m_{2}(x)$, the exact marginal distribution of
$p(x|s_i)$ is not Gaussian, which makes exact inference
intractable. To maintain a compact representation of offence and
defence skills, one can approximate $p(x|s_i)$ with a variational
Bayes framework or a sampling-based approach considering its being a univariate distribution.

\paragraph{\bf Bayesian update with VB}
In a variational Bayes framework, the problem is to choose a Gaussian distribution $q(x)^*:
\mathcal{N}(x;\mu_{\text{new}}, \sigma_{\text{new}}^2)$ that minimizes
the KL divergence between $p(x|s_i)$ and $q(x)$, i.e.,
\begin{align}\label{eq:marginal2}
    q(x)^* = \argmin_{q(x)} \text{KL}\left[q(x)|| p(x|s_i)\right].
\end{align}
%where $\text{KL}[p(x)||q(x)]=\int p(x)\log \frac{p(x)}{q(x)}dx$.
We derive a fixed-point approach for optimizing $q(x)$ \cite{Beal:EMFixedPoint02} and describe this approach below.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\noindent{\bf Minimizer $q(x)$ for $\text{KL}(q(x)||p(x|s_i))$:}
We first expand the KL-divergence into its definition:
\begin{align}\label{eq:KL}
    \text{KL}\left(q(x)|| p(x|s_i)\right) &= \int q(x) \log \left( \frac{q(x)}{p(x|s_i)} \right) dx \nonumber \\
%    &= -\left(\underbrace{-\int q(x) \log (q(x)) d(x)}_{\text{entropy of }q(x)}\right) - \int q(x) \log \left( p(x|s_i) \right) dx \nonumber \\
     \qquad &= -\log\sqrt{2\pi e \sigma_{new}^2} - E_{x\sim q(x)} \log \left( p(x|s_i) \right),
\end{align}
\unindentmore where $p(x|s_i)$ is the posterior probability of $x$ when
observing the score $s_i$. Since $q(x)$ is Gaussian
and the posterior has convenient Gaussian parts, manipulation of this
yields an equation for $\mu_{new}$ and $\sigma_{new}^2$ that can be
solved using an iterative fixed-point approach:
\begin{lemma}
Values for $\mu_{new}$ and $\sigma_{new}^2$
minimizing $\text{KL}\left(q(x)|| p(x|s_i)\right)$
satisfy
\begin{align}\label{eq:ExactMuSigmaNew}
    \mu_{\text{new}} & = \sigma^{2}\left(s_i - e^\kappa\right) + \mu, \nonumber \\
   \sigma_{\text{new}}^2 & = \frac{\sigma^2}{1+\sigma^2 e^\kappa },
\end{align}
where
\begin{align}\label{eq:approximationZQuad5}
     \kappa  &= \log\left(\frac{\mu + s_i\sigma^2-1-\kappa+\sqrt{(\kappa - \mu - s_i\sigma^2 -1)^2+2\sigma^2}}{2\sigma^2}\right).
\end{align}
\end{lemma}
\begin{proof}
The second term in~\eqref{eq:KL} is evaluated using
Bayes Theorem,
$p(x|s_i)=p(s_i|x)p(x)/p(s_i)$.
The term in $\log p(s_i)$ can be dropped because it is constant
with respect to $\mu_{\text{new}}$ and $\sigma_{\text{new}}^2$.
The term $E_{x\sim q(x)} [\log p(s_i|x)]$ is found by expanding the
Poisson distribution and noting $E_{x\sim p(x)}[\exp(x)] = \exp(\mu+\sigma^2/2)$ (Appendix~\ref{app:exponentialIntegral} for derivation).
Thus it becomes
\begin{equation}\label{eq:firstTermFinal}
  s_i \mu_{\text{new}} - \exp(\mu_{\text{new}} + \sigma_{\text{new}}^2/2) - \log(s_i!)~.
\end{equation}
\noindent
% Appendix \ref{app:exponentialIntegral} for derivation)
The term $E_{x\sim q(x)}[\log p(x)]$
according to the derivation in Appendix~\ref{app:logGaussianIntegral} becomes
\begin{equation}\label{eq:finalSecondTerm}
     -\frac{1}{2}\log(2\pi\sigma^2) -
         \frac{1}{2\sigma^2}\left(\sigma_{new}^2 + \mu_{new}^2-2\mu\mu_{new} + \mu^2 \right)~.
\end{equation}
Plugging~\eqref{eq:firstTermFinal} and~\eqref{eq:finalSecondTerm} into
\eqref{eq:KL} gives
\begin{align*}
    &\arg\min_{q(x)}\text{KL}\left(q(x)|| p(x|s_i)\right) \equiv \arg\min_{q(x)}-\log\sqrt{2\pi e \sigma_{new}^2} - \nonumber \\
    &\bigg( \underbrace{s_i \mu_{\text{new}} - \exp(\mu_{\text{new}} + \sigma_{\text{new}}^2/2) - \log(s_i!)}_{E_{x\sim q(x)} (\log p(s_i|x))} \nonumber \\
    & \underbrace{-\frac{1}{2}\log(2\pi\sigma^2) - \frac{1}{2\sigma^2}\left(\sigma_{new}^2 + \mu_{new}^2-2\mu\mu_{new} + \mu^2 \right)}_{E_{x\sim q(x)}(\log p(x))} \bigg).
\end{align*}
To find the minimizer $q(x)$, we calculate the partial derivatives of
$\text{KL}\left(q(x)|| p(x|s_i)\right)$ w.r.t.\
$\mu_{\text{new}}$ and $\sigma_{new}$, and set them to zero, leading to% as below
%{\small
%\begin{align*}
%    \partial{(\text{KL}\left(q(x)|| p(x|s_i)\right))}{\partial{\mu_{\text{new}}}} & = -s_i + \exp(\mu_{\text{new}}+\frac{\sigma_{\text{new}}^2}{2}) + \frac{\mu_{\text{new}}-\mu}{\sigma^2}, \\
%    \frac{\partial(\text{KL}\left(q(x)|| p(x|s_i)\right))}{\partial{\sigma_{\text{new}}}} &= -\frac{1}{\sigma_{\text{new}}} + \sigma_{\text{new}} \exp(\mu_{\text{new}}+\frac{\sigma_{\text{new}}^2}{2})+ \frac{\sigma_{\text{new}}}{\sigma^2}.
%\end{align*}}
%
%Setting the right hand sides of both partial derivatives to $0$ gives
\begin{align}
    \mu_{\text{new}} & = \sigma^{2}\left(s_i - \exp\left(\mu_{\text{new}}+\frac{\sigma_{\text{new}}^2}{2}\right)\right) + \mu, \nonumber \\
   \sigma_{\text{new}}^2 & = \frac{\sigma^2}{1+\sigma^2 \exp(\mu_{\text{new}}+\frac{\sigma_{new}^2}{2})}~.\nonumber
\end{align}
Summing the first plus half the second of these equations,
and defining $\kappa=\mu_{\text{new}}+\sigma_{\text{new}}^2/2$
yields the equation for $\kappa$ of
\begin{align}\label{eq:kappa}
    \kappa & = \mu + \sigma^2(s_i - \exp(\kappa)) + \frac{\sigma^2}{2(1+\sigma^2\exp(\kappa))},
\end{align}
and one gets~\eqref{eq:ExactMuSigmaNew} in terms of $\kappa$.
%Note that the variance $\sigma_{\text{new}}^2$ does not explicitly depends on $s_i$.

We convert~\eqref{eq:kappa} by solving for $\exp(\kappa)$
as it appears on the right-hand side.  This yields
a quadratic equation, and we take the positive solution since $\exp(\kappa)$
must be non-negative (see the Supplemental material).
The result gives us~\eqref{eq:approximationZQuad5}.
\end{proof}

We can use~\eqref{eq:approximationZQuad5}
as a fixed-point rewrite rule.
For a given $\mu$ and $\sigma^2$ together with an initial value of
$\kappa$, one iterates~\eqref{eq:approximationZQuad5} until
convergence.  Empirically, this happens within 2-3 iterations.
With convergence, we substitute the fixed-point solution
into \eqref{eq:ExactMuSigmaNew} to get the optimal mean
and variance for $q(x)^*$.

\paragraph{\bf Bayes Update with Slice Sampling}
One caveat associated with variational Bayes is that it may yield local minimum; and thus one tends to use sampling-based appoaches to obtain exact solutions. One widely used sampling-based approaches is based on Markov chain Monte Carlo (MCMC). To approximate this univariate distribution $p(x|s_i$, we can use one type of MCMC called slice sampling~\cite{neal03SliceSampling}. The idea behind slice sampling is to sample uniformly from the region under the plot of the density function to be approximated; by doing so, one can adatp to the characteristics of the distribution. One can construct this Markov chain that converges to this uniform distribution by alternating uniform sampling in the vertical direction with uniform sampling from the horizontal ``slice" defined by the current vertical position. This method can be easily implemented for univariate distributions, and we use the Matlab embedded function slicesample for approximating $p(x|s_i)$. 


\subsubsection{Win/Lose/Draw Probability}
\label{sec:winProbabilityPoisson}
Suppose we are given the offence and defence skills for team $i$ and $j$, we can pass the messages down in Figure~\ref{fig:trueskill_variant} to 
estimate the distributions for the performance variables $p_{oi}, p_{dj}, p_{di}, p_{oj}$. To estimate the win/lose/draw probability, we simply construct a  variable $d=p_{oi} + p_{di} - (p_{oj} + p_{dj})$ to represent the performance difference for team $i$ and $j$. Since the variable $d$ is a Gaussian distributed variable, the win/lose/draw probability $p(s>0)$for the Poisson-OD model defined below is simliar with the TrueSkill.
\begin{align}
  p(d>0) = 1 - \Phi\left(\frac{-\mu_d}{\sigma_d}\right),
\label{eq:winProbabilityPoisson}
\end{align}
where $\mu_d$ and $\sigma_d$ are the mean and the standard deviation of the performance difference variable $d$. We can omit the definitions of $\mu_d$ and $\sigma_d$ since they fall out of the linear operations of Gaussian variables. Given this Gaussian distribution performance difference variable, we define the draw and lose probability following that of TrueSkill defined in Section~\ref{sec:winProbabilityTrueSkill}. 

We pause here to note that the above definitions for win/lose/draw probability slightly deviates from the Poisson-OD model (Figure~\ref{fig:trueskill_variant}) in the sense that we do not compute these probabilities based on the score difference that the model predicts. To compute the winning probability of team $i$, i.e., $s_i>s_j$, based on score difference, we first construct a new variable $s= s_i-s_j$, the difference variable between two Poisson distributions, which proves to be a Skellam distribution in
\cite{Skellam46TheFrequencyDistribution}. Thus, we can compute the win
probability of $P(s>0)$ of team $i$, according to the probability mass
function for the Skellam distribution
\begin{align*}
     P(s=k; \lambda_i, \lambda_j) =e^{-(\lambda_i+\lambda_j)}\left(\frac{\lambda_i}{\lambda_j}\right)^{k/2}I_{|k|}\left(2\sqrt{\lambda_i\lambda_j}\right),
\end{align*}
where $I_{k}(z)$ is the modified Bessel function of the first kind given in \cite{Abramowitz74HandbookOfMathematical}.
\begin{align}
    I_k{(z)} = \left(\frac{z}{2}\right)^k\sum_{i=0}^{+\infty}\frac{(z^2/4)^i}{i!\Gamma(k+i+1)}.
\end{align}
Unfortunately, we need approximation to compute the win probability $P(s>0)$. To avoid the approximmation, we choose the Equation~\ref{eq:winProbabilityPoisson} for computing the winning probability in the present paper. The lose and draw probability are also derived from Equation~\ref{eq:winProbabilityPoisson} as described above. 

\subsection{Inference in Gaussian-OD Model}

\subsubsection{Bayesian update} 
In the Gaussian-OD model (Figure~\ref{fig:GaussianOD}), all
messages are Gaussian so one can
compute the belief update in closed-form as follows
\begin{align}
\label{eq:GaussianGraphicalModelsUpdatingEquation}
  \pi_{o_{i}}     &=    \frac{1}{\sigma_{o_{i}}^2} + \frac{1}{\beta_1^2+\beta_2^2+\gamma^2+\sigma_{d_{j}}^2},  \nonumber \\
  \tau_{o_{i}}    &=    \frac{\mu_{o_{i}}}{\sigma_{o_{i}}^2} + \frac{s_i+\mu_{d_{j}}}{\beta_1^2+\beta_2^2+\gamma^2+\sigma_{d_{j}}^2},  \nonumber \\
  \pi_{d_{j}}     &=    \frac{1}{\sigma_{d_{j}}^2} + \frac{1}{\beta_1^2+\beta_2^2+\gamma^2+\sigma_{o_{i}}^2}, \nonumber \\ 
  \tau_{d_{j}}    &=    \frac{\mu_{d_{j}}}{\sigma_{d_{j}}^2} + \frac{\mu_{o_{i}}-s_i}{\beta_1^2+\beta_2^2+\gamma^2+\sigma_{o_{i}}^2},
\end{align}
\unindentmore where $\mu_{o_{i}}$ and $\sigma_{o_{i}}$ are the mean
and standard deviation of the prior offence skill distribution of team
$i$, $\pi_{o_{i}} (\pi_{d_{j}}) = \frac{1}{\sigma_{\mathit{post}}^2}$
and $\tau_{o_{i}} (\tau_{d_{j}}) =
\frac{\mu_{\mathit{post}}}{\sigma_{\mathit{post}}^2}$ are the
precision and precision-adjusted mean for the posterior offence
(defence) skill distribution of team $i$ ($j$).  Likewise, one can
derive the update equations for team $j$'s offence skill $o_j$ and
team $i$'s defence skill $d_i$.

\subsubsection{Win/Lose/Draw Probability} 
To compute the probability of team $i$ winning vs team $j$, we first use message
passing to estimate the normally distributed distributions for score
variables $s_i$ and $s_j$, and then compute the probability that
$s_i-s_j>0$, i.e., team $i$'s score is larger than team $j$'s. Given
$s_i\sim\mathcal{N}(s_i;\mu_{si},\sigma_{si}^2)$ and
$s_j\sim\mathcal{N}(s_j;\mu_{sj},\sigma_{sj}^2)$, we can compute the
winning probability of team $i$ by
\begin{align}
  p(s>0) = 1 - \Phi\left(\frac{-(\mu_{si}-\mu_{sj})}{\sigma_{si}^2+\sigma_{sj}^2}\right).
\end{align}
where $s$ is the Gaussian distributed variable representing the difference $s_i-s_j$. The draw probability is the probaiblity density function of the Gaussian distributed variable $s$, evaluated at 0, and the lose probability of team $i$ is simply the probability that $s<0$, which is the normal cdf $\Phi\left(\frac{-(\mu_{si}-\mu_{sj})}{\sigma_{si}^2+\sigma_{sj}^2}\right)$. 

\subsection{Inference in Gaussian-SD Model}

\subsubsection{Bayes Update} 
In the Gaussian-SD model
(Figure~\ref{fig:modelAndInferenceGaussianGraphicalModelScoreDifference}),
all messages are Gaussian so we can again derive the update
for the single team skills $l_i$ and $l_j$ in closed-form
as follows:
\begin{align}
%\label{eq:GaussianGraphicalModelsUpdatingEquation}
  \pi_{l_{i}}  &=  \frac{1}{\sigma_{l_{i}}^2} + \frac{1}{\beta_1^2+\beta_2^2+\gamma^2+\sigma_{l_{j}}^2},  \nonumber \\
  \tau_{l_{i}} &=    \frac{\mu_{l_{i}}}{\sigma_{l_{i}}^2} + \frac{(s_i-s_j)+\mu_{l_{j}}}{\beta_1^2+\beta_2^2+\gamma^2+\sigma_{l_{j}}^2}, \nonumber    \\
  \pi_{l_{j}}  &=    \frac{1}{\sigma_{l_{j}}^2} + \frac{1}{\beta_1^2+\beta_2^2+\gamma^2+\sigma_{l_{i}}^2}, \nonumber \\
  \tau_{l_{j}} &=    \frac{\mu_{l_{j}}}{\sigma_{l_{j}}^2} + \frac{\mu_{l_{i}}-(s_i-s_j)}{\beta_1^2+\beta_2^2+\gamma^2+\sigma_{l_{i}}^2},
 \end{align}
where $\mu_{l_i}$ ($\mu_{l_j}$) and $\sigma_{l_i}$
      ($\sigma_{l_j}$) are the mean and standard deviation of team
      $i$'s (team $j$'s) prior skill distribution,
    % \item
$\pi_{l_{i}}$ ($\pi_{l_{j}}$) and $\tau_{l_{i}}$
      ($\tau_{l_{j}}$) are the precision and precision adjusted mean
      for team $i$'s (team $j$'s) posterior skill distribution.
    % \item $\mu_{l_j}$ and $\sigma_{l_j}$ are the mean and standard deviation of team $j$'s prior skill distribution,
    % \item $\pi_{l_{j}}$ and $\tau_{l_{j}}$ are the precision and precision adjusted mean for team $j$'s posterior skill distribution.
%\end{itemize}

\subsubsection{Win/Lose/Draw Probability} 
To estimate the winning probability of team $i$ for a match with team $j$, one can first use message passing to estimate the normally distributed score difference variable $s$, and then compute the winning probability of team $i$ by
\begin{align}
  p(s>0) = 1 -
  \Phi\left(\frac{l_i-l_j}{\sigma_i^2+\sigma_j^2+2 \beta^2}\right),
\end{align}
where $l_i$ and $\sigma_i$ are the mean and standard deviation for
team $i$'s skill level, and $\beta$ the standard deviation of the
performance variable.

%To estimate the winning probability of team $i$ for a match with team $j$, one can first use message passing to estimate the normally distributed score difference variable $s$, and then compute the winning probability of team $i$ by
%{\small
%\begin{align}
%  p(s>0) = 1 -
%  \Phi\left(\frac{l_i-l_j}{\sigma_i^2+\sigma_j^2+2 \beta^2}\right),
%\end{align}}
%where $l_i$ and $\sigma_i$ are the mean and standard deviation for
%team $i$'s skill level, and $\beta$ the standard deviation of the
%performance variable.
